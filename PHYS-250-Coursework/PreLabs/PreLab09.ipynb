{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Lab 09 : Eigenvalue Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "This prelab is split into two parts.  In the first part we will explore calculating eigenvalues and eigenvectors.  In the second part we will prepare for the lab.  The **second part of the prelab is essential for completing the lab**.  In the lab this week we will solve the Schrödinger equation.  This can be confusing if we are unfamiliar with the basics of quantum mechanics. The prelab will lead us through the set up making the lab much easier (and faster).  **It is in your best interest to understand this setup prior to the lab.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "As always you should add initialization to the top of your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "949337af202bf4f2530d8462a7458d7f",
     "grade": true,
     "grade_id": "cell-7fb06d8f89a674b3",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy.optimize as opt\n",
    "import scipy.interpolate as interp\n",
    "import scipy.special as sf\n",
    "import scipy.integrate as integ\n",
    "import scipy.linalg as la\n",
    "import matplotlib as mpl\n",
    "mpl.rc('xtick', direction='in', top=True)\n",
    "mpl.rc('ytick', direction='in', right=True)\n",
    "mpl.rc('xtick.minor', visible=True)\n",
    "mpl.rc('ytick.minor', visible=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalues and Eigenvectors\n",
    "\n",
    "\n",
    "In homework 8, problem 2 we encountered a system of linear equations represented by the matrix\n",
    "$$\n",
    "\\mathsf{A} = \\begin{pmatrix}\n",
    "4 & -7 & 3 \\\\\n",
    "1 & 3 & -3 \\\\\n",
    "3 & -29 & 21\n",
    "\\end{pmatrix}.$$\n",
    "\n",
    "We should have seen that `scipy.linalg.solve` did provide a solution to the system of equations despite this system being linearly dependent.  Here we will study this matrix in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a system of linear equations is not linearly independent, then the matrix representing this system should be singular.  Mathematically, what does this mean for the determinant of the matrix? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "6202eedc009aee7b1d636b39ccc0cb0f",
     "grade": true,
     "grade_id": "cell-7eec247b35a37cbf",
     "locked": false,
     "points": 0.25,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "The determinant is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the matrix as a NumPy array and numerically calculate and print its determinant. Notice how this compares to the expectation: it should agree to within numerical precision errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "72a702ea979f3c6773cef1ab1047e8ce",
     "grade": true,
     "grade_id": "cell-6ef28701e8472bcc",
     "locked": false,
     "points": 0.25,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.101481059543793e-14\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[4,-7,3],\n",
    "             [1,3,-3],\n",
    "             [3,-29,21]])\n",
    "print(la.det(A))\n",
    "assert(np.allclose(0, la.det(A)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate and print the inverse of the matrix, $\\mathsf{A}^{-1}$, using `scipy.linalg.inv`.  Store the inverse in a variable as we will use it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7142a812a7dad67536c829eaed9ea9cb",
     "grade": true,
     "grade_id": "cell-8b2552093e71a00c",
     "locked": false,
     "points": 0.25,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "A_inv = la.inv(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the result from the previous part comment on why using $\\mathsf{A}^{-1}$ in calculations will not produce numerically accurate results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "04170906515d9ac49b6afac09d85ec3d",
     "grade": true,
     "grade_id": "cell-2662044c8801267c",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "We know analytically that $A^{-1}$ does not exist. Numerically this leads to an $A^{-1}$ that has rows of zeros or values very close to zero. If this matrix is used the values close to zero can lead to false results if they are used in division for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the inverse calculated above is not really the inverse.  Do this by calculating both $\\mathsf{A} \\mathsf{A}^{-1}$ and $\\mathsf{A}^{-1} \\mathsf{A}$.  Compare these to the identity matrix, $\\mathsf{I}$.  (*Note:* You can construct the identity matrix using `np.eye` or `np.identity` if you are so inclined.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e2f4058bd8240949d2aac7eba0aa18fa",
     "grade": true,
     "grade_id": "cell-7f919fd9cc7db49c",
     "locked": false,
     "points": 0.25,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.5   -1.     0.125]\n",
      " [ 0.5   -1.    -0.125]\n",
      " [ 6.5    5.    -0.625]]\n",
      "[[ 1.125 -0.875 -0.125]\n",
      " [-0.125 -3.125  0.125]\n",
      " [ 0.625 -2.375 -0.625]]\n"
     ]
    }
   ],
   "source": [
    "print(A@A_inv)\n",
    "print(A_inv@A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly decribe the results from the previous part and what they mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5bcc9a38ccd47d1ac7ef7b93269b3cc1",
     "grade": true,
     "grade_id": "cell-90bcd617071a6d51",
     "locked": false,
     "points": 0.25,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "This means that the inverse calculated by `scipy.linalg.inv` is not the inverse. This is expected because $A$ does not have an inverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen some of the problems with this matrix, let us look at its eigenvalues and eigenvectors.  We will discover that we *can reliably calculate* the eigenvalues and eigenvectors of a singular matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate and print the eigenvalues and the eigenvectors of the matrix $\\mathsf{A}$.  In what follows I will call the matrix of eigenvectors $\\mathsf{B}$. (*Note:* For comparison purposes the determinant of a matrix is the product of the eigenvalues of the matrix, that is \n",
    "$$\\det(\\mathsf{A}) = \\prod_{j=1}^{N} \\lambda_j$$\n",
    "where the $\\lambda_j$ are the eigenvalues of the $N\\times N$ matrix $\\mathsf{A}$.  Mathematically this means for a singular matrix that at least one of the eigenvalues must be zero.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3604d0a28c64ee92131700e6f853c3d4",
     "grade": true,
     "grade_id": "cell-7e182a6f9f486d8d",
     "locked": false,
     "points": 0.25,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The eigenvalues are [2.22044605e-15+0.j 2.77502784e+00+0.j 2.52249722e+01+0.j]\n",
      "The eigenvectors are [[ 0.44413993  0.89948293  0.17876267]\n",
      " [ 0.55517491  0.29539723 -0.12371167]\n",
      " [ 0.70322155  0.3219797   0.97608367]]\n"
     ]
    }
   ],
   "source": [
    "ew, ev = la.eig(A)\n",
    "print(\"The eigenvalues are \" + str(ew))\n",
    "print(\"The eigenvectors are \" + str(ev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix of eigenvectors should diagonalize the matrix $\\mathsf{A}$, in other words, $\\mathsf{B}^{-1} \\mathsf{A} \\mathsf{B}$ should equal a diagonal matrix with the eigenvalues along the diagonal.  Verify this is true (to within machine precision error).  (*Note:* To construct a diagonal matrix from a vector you may use `np.diag`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "40516f7b22c4db7dc80d128881cbc262",
     "grade": true,
     "grade_id": "cell-43e31db9d51c0494",
     "locked": false,
     "points": 0.25,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "assert(np.allclose(la.inv(ev)@A@ev, np.diag(ew)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schrödinger Equation\n",
    "\n",
    "In the lab this week we will solve the Schrödinger equation in one dimension for two different potentials.\n",
    "\n",
    "Mathematically, much of quantum mechanics reduces to solving eigenvalue problems.  We have probably encountered the time independent Schrödinger equation in modern physics.  In its shortest form it may be written as $\\hat H \\psi(x)=E\\psi(x)$.  Here $\\hat H$ is the Hamiltonian operator which we will have more to say about, $E$ is the energy of the system, and $\\psi(x)$ is the wave function which encodes the information about the state of the system.  The Hamiltonian is determined from the physics, so, given a Hamiltonian, our goal is to solve for the energy and the wave function.\n",
    "\n",
    "When we first study quantum mechanics we typically write the Hamiltonian as a differential operator.  For a single particle in one dimension the nonrelativistic Hamiltonian may be written as\n",
    "$$\\hat H = -\\frac{\\hbar^2}{2m} \\frac{\\mathrm{d}^2}{\\mathrm{d}x^2}+V(x).$$\n",
    "Here $\\hbar=h/2\\pi$ is the reduced Planck constant and $V(x)$ is the potential energy (though we will just refer to it as a potential).  It is convenient to work in units of $\\hbar$ and $m$ meaning that they get absorbed into the energies.  In this case the Schrödinger equation becomes\n",
    "$$ -\\frac12 \\frac{\\mathrm{d}^2 \\psi(x)}{\\mathrm{d}x^2}+V(x)\\psi(x)=E\\psi(x).$$\n",
    "\n",
    "In a quantum mechanics course we solve this equation for various choices of the potential energy.  To solve it numerically we can proceed in a number of ways.  One is to use a differential equation integrator, such as `scipy.integrate.solve_ivp`, to do the work for us.  (Since this is such an important equation, specialized integrators have been developed which would be even better to use.)  **An alternative approach** is to instead turn it into a matrix equation and solve it as an eigenvalue problem.\n",
    "\n",
    "As we very briefly mentioned when discussing numerical derivatives, we can discretize the second derivative using\n",
    "$$ \\frac{\\mathrm{d}^2 \\psi(x)}{\\mathrm{d}x^2} = \\frac{\\psi(x-h) - 2 \\psi(x) + \\psi(x+h)}{h^2}.$$\n",
    "Note that here $h$ is the step size and **has nothing to do with Planck’s constant**.  Despite the fact that this algorithm is only accurate to order $h^2$ it will be sufficient for our purposes.  With this we can rewrite the Schrödinger equation in matrix form\n",
    "\n",
    "$$ \\mathsf{H}\\vec\\psi = E\\vec\\psi, $$\n",
    "\n",
    "where $\\mathsf{H}$ is now **a matrix representing our Hamiltonian** and $\\vec\\psi$ is now **a vector representing our wave function**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Linear System\n",
    "\n",
    "We need to understand the linear system we just wrote down. The approach we will use and the structure we will find is very similar to that from PreLab 07.  To this end it is best to again grab some paper in order to work out some of the details.  We are again solving a one dimensional system in a region of length $L$ along the $x$-axis.  To do so, we discretize the wave function by evaluating it at $N+1$ points. Further, we will take these points to be equally spaced with step size $h$ along the $x$-axis so that $\\psi_j \\equiv \\psi(x_j)$ with $x_j = x_0 + j h$.  With this and the discretized form of the second derivative we can rewrite $\\hat H \\psi(x)$ in the discrete (matrix) form $\\mathsf{H}\\vec \\psi$.  Use this to find the form of $\\mathsf{H}$.  You should find it is a symmetric tridiagonal matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in PreLab 07 it is best to begin with a small system.  Again let $N=4$ (so that we have 5 points).  Write out the 5 equations, one for each of the components $\\psi_j$, and use this to construct the matrix $\\mathsf{H}$.  Fill in the components of this matrix below.  For the boundary conditions let $\\psi_{-1} = \\psi_{N+1} = 0$.  Notationally it is also convenient to write $V_j\\equiv V(x_j)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For your convenience, here is a template you can use to write the matrix.  Cut and paste this into the solution cell, then replace the dots with the appropriate expressions.\n",
    "\n",
    "$$ \\mathsf{H} = \\begin{pmatrix}\n",
    ". & . & . & . & . \\\\\n",
    ". & . & . & . & . \\\\\n",
    ". & . & . & . & . \\\\\n",
    ". & . & . & . & . \\\\\n",
    ". & . & . & . & . \n",
    "\\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "51af967caa4443a2c518054211edcd72",
     "grade": true,
     "grade_id": "cell-d0d6b3e8ad84d7e5",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "$$ \\mathsf{H} = \\begin{pmatrix}\n",
    "\\frac{1}{h^2}+V_0 & \\frac{-1}{2h^2} & 0 & 0 & 0 \\\\\n",
    "\\frac{-1}{2h^2} & \\frac{1}{h^2}+V_1 & \\frac{-1}{2h^2} & 0 & 0 \\\\\n",
    "0 & \\frac{-1}{2h^2} & \\frac{1}{h^2}+V_2 & \\frac{-1}{2h^2} & 0 \\\\\n",
    "0 & 0 & \\frac{-1}{2h^2} & \\frac{1}{h^2}+V_3 & \\frac{-1}{2h^2} \\\\\n",
    "0 & 0 & 0 & \\frac{-1}{2h^2} & \\frac{1}{h^2}+V_4 \n",
    "\\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalue Problem\n",
    "\n",
    "Now that we know $\\mathsf{H}$ we can solve the eigenvalue problem $\\mathsf{H}\\vec\\psi = E\\vec\\psi$ for $E$ and $\\psi$.  Since $\\mathsf{H}$ is a tridiagonal matrix it is again best to use a specialized function for this case.  Thus instead of using `scipy.linalg.eig` or `scipy.linalg.eigh` we will use `scipy.linalg.eig_banded`.  Look up its documentation.  You should see it is similar to what we encountered in the previous Lab except that it **requires a symmetric banded matrix**.  Fortunately that is exactly what we have.  In fact, this function is more similar to `scipy.linalg.solveh_banded`. \n",
    "\n",
    "It is easiest to use the function in its default configuration.  This means it will return both the eigenvalues and eigenvectors for $\\mathsf{H}$ given the **upper** part of the matrix.  Once again study the documentation and pay particular attention to the *upper form* in the example.  There they show an example of a $6\\times6$ matrix with 5 \"diagonals\" (2 upper and 2 lower bands along with the main diagonal.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide code for constructing the matrix $\\mathsf{H}$ in banded form as used by `scipy.linalg.eig_banded`.  Your code should work for arbitrary $N$, length $L$, and potential $V(x)$.\n",
    "\n",
    "Write your function for constructing the Hamiltonian matrix below.\n",
    "\n",
    "(*Note:* There are some elements of the matrix that are not used by the algorithm.  It is best to fill these in anyway.  In fact, it is easier to construct the matrix by not treating these elements as special.  Since there values do not matter, just fill them in along with the rest of the band in which they fall.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "80b05c5b234e0601ed3445d194029817",
     "grade": true,
     "grade_id": "cell-2b7464d09bcbe9f2",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def Hamiltonian (N, L, V) :\n",
    "    \"\"\"Constructs a Hamiltonian matrix in a form to be used in scipy.linalg.eig_banded \n",
    "    Parameters\n",
    "    ----------\n",
    "    N : number of steps\n",
    "    L : length\n",
    "    V : function, the potential\"\"\"\n",
    "    #Construct a matrix of proper dimensions\n",
    "    H = np.zeros((2, N+1))\n",
    "    \n",
    "    #Calculate step size and fill in matrix\n",
    "    h = L/N\n",
    "    H[0] = -1/(2*h**2)\n",
    "    #Make array of potential values\n",
    "    x = np.linspace(0, L, N+1)\n",
    "    V_arr = V(x)\n",
    "    #Fill in last row of H\n",
    "    H[1] = 1/h**2 + V_arr\n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple test case, if the potential is a constant, for example, $V(x)=1$, and we choose $L=1$ and $N=4$, then the matrix is simple to calculate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9ffb5355b703fd06486a519008c5f0f5",
     "grade": true,
     "grade_id": "cell-cfee6721fc00e1a0",
     "locked": true,
     "points": 0.5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert(np.allclose(Hamiltonian(4, 1, lambda x : 1), \n",
    "                   np.array([[-8., -8., -8., -8., -8.],\n",
    "                             [17., 17., 17., 17., 17.]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Unfortunately we are not quite done.  By convention when we solve an eigenvalue problem the eigenvectors are returned as unit vectors.  This means that when we solve the Schrödinger equation as an eigenvalue problem the eigenvectors will satisfy $|\\vec\\psi|^2 = 1$.  However, in quantum mechanics to allow a probabilistic interpretation of the wave function we instead require the integral condition\n",
    "\n",
    "$$ \\int |\\psi(x)|^2 \\mathrm{d}x = 1.  $$\n",
    "\n",
    "(Here the integral is over the entire region where $\\psi(x)\\ne 0$.)  **These are not the same conditions.**  This means we need to renormalize the eigenvector to be consistent with the requirement from quantum mechanics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an eigenvector, $\\vec\\psi$, how must we modify it to be consistent with the normalization condition from quantum mechanics?  To determine this it is simplest to discretize the integral condition from quantum mechanics.  Here it is sufficient to use the Riemann sum form of the integral from calculus, with equally spaced points, and see what rescaling is required in order that the integral condition is satisfied.  In other word, solving the eigenvalue problem gives us a vector, let us call it $\\vec\\psi^{(\\mathrm{ev})}$, that satisfies $|\\vec\\psi^{(\\mathrm{ev})}|^2=1$.  The state relevant for quantum mechanics is proportional to this, $\\vec\\psi = \\alpha \\vec\\psi^{(\\mathrm{ev})}$.  Our objective is to find $\\alpha$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "6b698b958eae6c0e9692fcd1309157a3",
     "grade": true,
     "grade_id": "cell-6b246fa585ecd6ec",
     "locked": false,
     "points": 0.25,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "For our wave function to be normalized the following equation must be satistified:\n",
    "$$\\sum_{j=0}^{N} |\\alpha\\psi_j|^2h = 1. $$\n",
    "This imples\n",
    "$$\\alpha = \\frac{1}{\\sqrt{h\\sum_{j=0}^{N} |\\psi_j|^2}}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning in the PreLab\n",
    "\n",
    "All prelabs will be handled as was done for PreLab01.  See that file for details.  It will be assumed from now on that you have read and understood the procedure and what it means when you submit a prelab."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "authors": [
   {
    "name": "Craig J Copi",
    "semester": "Spring 2019"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
