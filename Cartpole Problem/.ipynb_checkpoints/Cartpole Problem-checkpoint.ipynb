{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Cartpole Problem\n",
    "The goal of the cartpole problem is to keep an inverted pendulum attached to a cart on a frictionless track upright by exerting forces on the cart. I will explore both physics based solutions and solutions found from reinforcement learning. I begin with a physical solution which picks a force that minimizes the pole angulary velocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gym\n",
    "# env = gym.make('CartPole-v0')\n",
    "# env.reset()\n",
    "# for _ in range(500):\n",
    "#     env.render()\n",
    "#     env.step(env.action_space.sample()) # take a random action\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average reward of the physical solution is 192.44.\n",
      "The standard deviation of the reward of the physical solution is 13.315644933686091.\n"
     ]
    }
   ],
   "source": [
    "from itertools import permutations\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "from time import time\n",
    "\n",
    "def theta_dot_minimizer(observation, env):\n",
    "    \"\"\"\n",
    "    A physics based solution to the cartpole environment from the gym module.\n",
    "    Parameters\n",
    "    ----------\n",
    "    observation (ndarray): contains the cart position, cart velocity, pole angle, and pole angular velocity\n",
    "    env (object): the cartpole environment being used\n",
    "    Returns\n",
    "    -------\n",
    "    force (int): either 0 or 1 to represent a leftward or rightward force on the cart\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get initial conditions from environment\n",
    "    x, x_dot, theta, theta_dot = env.state\n",
    "    \n",
    "    # Create an array with all possible forces\n",
    "    force = np.array([1, -1])\n",
    "    \n",
    "    # Compute sine and cosine of pole angle\n",
    "    costheta = np.cos(theta)\n",
    "    sintheta = np.sin(theta)\n",
    "    \n",
    "    # Calculate the pole angular acceleration\n",
    "    temp = (force + env.polemass_length * theta_dot ** 2 * sintheta) / env.total_mass\n",
    "    thetaacc = (env.gravity * sintheta - costheta * temp) / (env.length * (4.0 / 3.0 - env.masspole * costheta ** 2 / env.total_mass))\n",
    "\n",
    "    # Calculate pole angular velocity at next time step using Euler's method\n",
    "    theta_dot = theta_dot + env.tau * thetaacc\n",
    "    \n",
    "    # Rerturn the force that minimizes absolute value of theta_dot\n",
    "    if np.abs(theta_dot[0]) < np.abs(theta_dot[1]):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "# env_to_wrap = gym.make('CartPole-v0')\n",
    "# env = wrappers.Monitor(env_to_wrap, './videos/' + str(time()) + '/')\n",
    "# observation = env.reset()\n",
    "# done = False\n",
    "\n",
    "# while not done:\n",
    "#     env.render()\n",
    "#     observation, reward, done, info = env.step(env.action_space.sample())\n",
    "# env.close()\n",
    "\n",
    "# t = 0\n",
    "# while True:\n",
    "#     t += 1\n",
    "#     env.render()\n",
    "#     action = theta_dot_minimizer(observation, env)\n",
    "#     observation, reward, done, info = env.step(action)\n",
    "#     if done:\n",
    "#         print(\"Episode finished after {} timesteps\".format(t))\n",
    "#         break\n",
    "# env.close()\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "total_games = 100\n",
    "rewards = np.zeros(total_games)\n",
    "for i in range(total_games):\n",
    "    total_reward = 0\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    while not done:       \n",
    "        action = theta_dot_minimizer(observation, env)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            done = False\n",
    "            rewards[i] = total_reward\n",
    "            break\n",
    "\n",
    "print(f'The average reward of the physical solution is {np.average(rewards)}.')\n",
    "print(f'The standard deviation of the reward of the physical solution is {np.std(rewards)}.')\n",
    "\n",
    "# env_to_wrap.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I solve the envrionment with the REINFORCE algo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-bfd4cece5a6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mreinforce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mreinforce\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Google Drive (bossm@oregonstate.edu)\\Cartpole Problem\\reinforce.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m(alpha, GAMMA)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m                 \u001b[0mupdate_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mGAMMA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m                 \u001b[0mnumsteps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m                 \u001b[0mavg_numsteps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumsteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Google Drive (bossm@oregonstate.edu)\\Cartpole Problem\\reinforce_update.py\u001b[0m in \u001b[0;36mupdate_policy\u001b[1;34m(policy_network, rewards, log_probs, alpha, GAMMA)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mpolicy_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mpolicy_gradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy_gradient\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mpolicy_gradient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mpolicy_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import reinforce\n",
    "reinforce.main(alpha=.5, GAMMA=.9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
